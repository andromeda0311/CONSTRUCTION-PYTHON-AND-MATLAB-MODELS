{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEPFS Budget Prediction Workflow (End-to-End)\n",
    "\n",
    "This notebook consolidates the entire workflow for the MEPFS budget prediction model, including data merging, initial visualization, data processing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Merging (from `csv_merger.py`)\n",
    "\n",
    "First, we merge the quantity and unit cost CSV files to create a total cost file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_project_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return name\n",
    "    original_name = name\n",
    "    name_upper = name.upper()\n",
    "    sty_cl_match = re.search(r'(\\d+)\\s*STY\\s*(\\d+)\\s*CL(S)?', name_upper)\n",
    "    if sty_cl_match:\n",
    "        floors = sty_cl_match.group(1)\n",
    "        rooms = sty_cl_match.group(2)\n",
    "        rest_of_name = original_name[sty_cl_match.end():].strip()\n",
    "        return f\"{floors} STY {rooms} CLS {rest_of_name}\"\n",
    "    x_match = re.search(r'(\\d+)\\s*X\\s*(\\d+)', name_upper)\n",
    "    if x_match:\n",
    "        floors = x_match.group(1)\n",
    "        rooms = x_match.group(2)\n",
    "        rest_of_name = original_name[x_match.end():].strip()\n",
    "        return f\"{floors} STY {rooms} CLS {rest_of_name}\"\n",
    "    return original_name\n",
    "\n",
    "def extract_year_and_budget(entry):\n",
    "    if not isinstance(entry, str):\n",
    "        return None, None\n",
    "    cleaned_entry = re.sub(r'(\\d{4})\\.', r'\\1: ', entry, count=1)\n",
    "    match = re.match(r'(\\d{4}|\\d{2}//):\\s*(.*)', cleaned_entry.strip())\n",
    "    if match:\n",
    "        year_str = match.group(1).replace('//', '00')\n",
    "        budget_str = match.group(2).replace(',', '')\n",
    "        if budget_str.count('.') > 1:\n",
    "            parts = budget_str.split('.')\n",
    "            budget_str = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "        try:\n",
    "            budget = float(budget_str)\n",
    "            return year_str, budget\n",
    "        except (ValueError, TypeError):\n",
    "            return year_str, None\n",
    "    try:\n",
    "        budget_str = cleaned_entry.replace(',', '')\n",
    "        if budget_str.count('.') > 1:\n",
    "            parts = budget_str.split('.')\n",
    "            budget_str = \"\".join(parts[:-1]) + \".\" + parts[-1]\n",
    "        budget = float(budget_str)\n",
    "        return None, budget\n",
    "    except (ValueError, TypeError):\n",
    "        return None, None\n",
    "\n",
    "def process_cost_files(quantity_file, unit_cost_file, output_file):\n",
    "    quantity_df = pd.read_csv(quantity_file)\n",
    "    unit_cost_df = pd.read_csv(unit_cost_file)\n",
    "    quantity_df.rename(columns={quantity_df.columns[0]: 'Project', quantity_df.columns[1]: 'Year_Budget'}, inplace=True)\n",
    "    unit_cost_df.rename(columns={unit_cost_df.columns[0]: 'Project', unit_cost_df.columns[1]: 'Year_Budget'}, inplace=True)\n",
    "    quantity_df['Original_Project'] = quantity_df['Project']\n",
    "    year_budget_info = quantity_df['Year_Budget'].apply(extract_year_and_budget).apply(pd.Series)\n",
    "    quantity_df['Year'] = year_budget_info[0]\n",
    "    quantity_df['Budget'] = year_budget_info[1]\n",
    "    quantity_df.set_index('Original_Project', inplace=True)\n",
    "    unit_cost_df.set_index('Project', inplace=True)\n",
    "    start_col_index_qty = quantity_df.columns.get_loc('MEPFS aspect') + 1\n",
    "    numeric_quantity = quantity_df.iloc[:, start_col_index_qty:-2].copy()\n",
    "    start_col_index_unit = unit_cost_df.columns.get_loc('MEPFS aspect') + 1\n",
    "    numeric_unit_cost = unit_cost_df.iloc[:, start_col_index_unit:].copy()\n",
    "    numeric_unit_cost = numeric_unit_cost.reindex(columns=numeric_quantity.columns)\n",
    "    for col in numeric_quantity.columns:\n",
    "        numeric_quantity[col] = pd.to_numeric(numeric_quantity[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)\n",
    "    for col in numeric_unit_cost.columns:\n",
    "        numeric_unit_cost[col] = pd.to_numeric(numeric_unit_cost[col].astype(str).str.replace(',', ''), errors='coerce').fillna(0)\n",
    "    total_cost_df = numeric_quantity.multiply(numeric_unit_cost)\n",
    "    final_df = quantity_df[['Project', 'Year', 'Budget']].copy()\n",
    "    final_df['Project'] = final_df['Project'].apply(standardize_project_name)\n",
    "    final_df = final_df.join(total_cost_df)\n",
    "    final_df.reset_index(drop=True, inplace=True)\n",
    "    cols = ['Project', 'Year', 'Budget'] + [col for col in final_df if col not in ['Project', 'Year', 'Budget']]\n",
    "    final_df = final_df[cols]\n",
    "    # Clean up unnecessary columns\n",
    "    final_df = final_df.loc[:, ~final_df.columns.str.contains('^Unnamed')]\n",
    "    final_df.dropna(axis=1, how='all', inplace=True)\n",
    "    print(f'Cleaned columns. Remaining columns: {final_df.columns.tolist()}')\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully created the merged file: {output_file}\")\n",
    "    return final_df\n",
    "\n",
    "quantity_filename = 'MEPFS Quantity Cost.csv'\n",
    "unit_cost_filename = 'MEPFS Unit Cost.csv'\n",
    "total_cost_filename = 'MEPFS_Total_Cost.csv'\n",
    "total_cost_df = process_cost_files(quantity_filename, unit_cost_filename, total_cost_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initial Data Visualization (from `mepfs_visualizer.py`)\n",
    "\n",
    "Now, let's visualize the distributions of the merged data to check for outliers and skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distributions(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    cost_columns = [col for col in df.columns if col not in ['Year', 'Project']]\n",
    "    for col in cost_columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col].dropna(), kde=True)\n",
    "        plt.title(f'Histogram of {col}')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[col].dropna())\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "        safe_col_name = re.sub(r'[^a-zA-Z0-9_]', '_', col)\n",
    "        filename = os.path.join(output_dir, f'{safe_col_name}_initial_visualization.png')\n",
    "        plt.savefig(filename)\n",
    "        plt.show()\n",
    "        print(f\"Saved initial visualization for {col} to {filename}\")\n",
    "\n",
    "VISUALIZATION_DIR = 'visualizations'\n",
    "visualize_distributions(total_cost_df, VISUALIZATION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing (from `mepfs_data_processing.py`)\n",
    "\n",
    "Next, we clean the merged data and filter out irrelevant rows for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_transform_data(df, output_filepath):\n",
    "    df_processed = df.copy()\n",
    "    cost_columns = [col for col in df_processed.columns if col not in ['Year', 'Project']]\n",
    "    for col in cost_columns:\n",
    "        df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
    "    df_processed.dropna(subset=cost_columns, inplace=True)\n",
    "    df_processed = df_processed[df_processed['Budget'] > 0]\n",
    "    feature_columns = [col for col in df_processed.columns if col not in ['Year', 'Budget', 'Project']]\n",
    "    zero_features_mask = (df_processed[feature_columns] == 0).all(axis=1)\n",
    "    df_processed = df_processed[~zero_features_mask]\n",
    "    \n",
    "    # Remove duplicates - round numeric columns to avoid floating point precision issues\n",
    "    rows_before = len(df_processed)\n",
    "    for col in cost_columns:\n",
    "        df_processed[col] = df_processed[col].round(2)\n",
    "    df_processed.drop_duplicates(inplace=True)\n",
    "    rows_after = len(df_processed)\n",
    "    print(f\"Removed {rows_before - rows_after} duplicate rows\")\n",
    "    \n",
    "    df_processed = df_processed.drop(columns=['Project'])\n",
    "    df_processed.to_csv(output_filepath, index=False)\n",
    "    print(f\"\\nPreprocessed data saved to {output_filepath}\")\n",
    "    return df_processed\n",
    "\n",
    "PREPROCESSED_DATA_PATH = 'mepfs_preprocessed_data.csv'\n",
    "preprocessed_df = clean_and_transform_data(total_cost_df, PREPROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering and Visualization\n",
    "\n",
    "Before training the model, we create additional features that might capture more complex relationships in the data. We also apply a log transformation to the 'Budget' target variable to handle its likely right-skewed distribution, which is common for monetary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_featured = preprocessed_df.copy()\n",
    "\n",
    "# Feature Engineering\n",
    "# 1. Total cost of all MEPFS components\n",
    "feature_columns = [col for col in df_featured.columns if col not in ['Year', 'Budget']]\n",
    "df_featured['Total_Features_Cost'] = df_featured[feature_columns].sum(axis=1)\n",
    "\n",
    "# 2. Count of non-zero MEPFS components\n",
    "df_featured['Non_Zero_Features'] = (df_featured[feature_columns] > 0).sum(axis=1)\n",
    "\n",
    "# 3. Average cost per non-zero component\n",
    "# Adding a small epsilon to avoid division by zero\n",
    "epsilon = 1e-6\n",
    "df_featured['Avg_Cost_Per_Feature'] = df_featured['Total_Features_Cost'] / (df_featured['Non_Zero_Features'] + epsilon)\n",
    "\n",
    "print(\"New features created: 'Total_Features_Cost', 'Non_Zero_Features', 'Avg_Cost_Per_Feature'\")\n",
    "\n",
    "# Visualize correlations with new features before log transformation\n",
    "def plot_correlation_matrix(df, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    # Note: Using the original 'Budget' values for correlation interpretability\n",
    "    correlation_matrix = df.drop('Year', axis=1, errors='ignore').corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 8})\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(output_dir, 'feature_correlation_matrix.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(f\"Correlation matrix saved to {filename}\")\n",
    "\n",
    "plot_correlation_matrix(df_featured, VISUALIZATION_DIR)\n",
    "\n",
    "# Log transform the target variable 'Budget' to handle skewness\n",
    "# We use np.log1p which is equivalent to log(1+x) to handle potential zero values\n",
    "df_featured['Budget'] = np.log1p(df_featured['Budget'])\n",
    "print(\"\\n'Budget' column has been log-transformed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Model Training with Hyperparameter Tuning\n",
    "\n",
    "Here, we replace the manual LightGBM training process with `GridSearchCV`. This not only tests different combinations of hyperparameters to find the best ones but also uses K-fold cross-validation to give a more reliable measure of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(y_true, y_pred, output_dir, data_set_name='Test'):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--r', linewidth=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'Actual vs. Predicted Values ({data_set_name} Set)')\n",
    "    plt.grid(True)\n",
    "    filename = os.path.join(output_dir, f'{data_set_name.lower()}_actual_vs_predicted.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "    print(f\"Actual vs. Predicted plot for {data_set_name} set saved to {filename}\")\n",
    "\n",
    "def train_budget_prediction_model_advanced(df, model_output_dir, visualization_dir):\n",
    "    \"\"\"\n",
    "    Trains the model using GridSearchCV for hyperparameter tuning and cross-validation.\n",
    "    \"\"\"\n",
    "    # Using the new engineered features\n",
    "    X = df.drop('Budget', axis=1)\n",
    "    y = df['Budget']\n",
    "    if 'Year' in X.columns:\n",
    "        X = X.drop('Year', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"Features: {', '.join(X.columns)}\")\n",
    "    print(f\"Data split into training ({len(X_train)} rows) and testing ({len(X_test)} rows) sets.\")\n",
    "\n",
    "    # --- Hyperparameter Tuning with GridSearchCV ---\n",
    "    estimator = lgb.LGBMRegressor(objective='regression', metric='l2', random_state=42)\n",
    "    \n",
    "    # Define a parameter grid to search. This can be expanded.\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [200, 500, 1000],\n",
    "        'num_leaves': [20, 31, 40],\n",
    "        'reg_alpha': [0.1, 0.5],  # L1 regularization\n",
    "        'reg_lambda': [0.1, 0.5]  # L2 regularization\n",
    "    }\n",
    "    \n",
    "    # Use 5-fold cross-validation\n",
    "    gsearch = GridSearchCV(estimator=estimator, param_grid=param_grid, scoring='r2', n_jobs=-1, cv=5, verbose=1)\n",
    "    \n",
    "    print(\"\\nStarting Hyperparameter Tuning...\")\n",
    "    grid_result = gsearch.fit(X_train, y_train)\n",
    "    print(\"Hyperparameter Tuning Complete.\")\n",
    "    \n",
    "    print(f\"\\nBest R² Score from CV: {grid_result.best_score_:.2%}\")\n",
    "    print(f\"Best Parameters: {grid_result.best_params_}\")\n",
    "    \n",
    "    # Use the best estimator found by GridSearchCV\n",
    "    best_model = grid_result.best_estimator_\n",
    "    \n",
    "    # --- Evaluate the Final Model ---\n",
    "    # Remember to transform predictions back from log scale using np.expm1\n",
    "    y_train_pred_log = best_model.predict(X_train)\n",
    "    y_train_pred = np.expm1(y_train_pred_log)\n",
    "    # Transform y_train back as well for evaluation\n",
    "    y_train_actual = np.expm1(y_train)\n",
    "    train_r2 = r2_score(y_train_actual, y_train_pred)\n",
    "    train_mae = mean_absolute_error(y_train_actual, y_train_pred)\n",
    "    print(f\"\\nFinal Training Accuracy (R²): {train_r2:.2%}\")\n",
    "    print(f\"Final Training MAE: {train_mae:.2f}\")\n",
    "    \n",
    "    y_test_pred_log = best_model.predict(X_test)\n",
    "    y_test_pred = np.expm1(y_test_pred_log)\n",
    "    # Transform y_test back as well for evaluation\n",
    "    y_test_actual = np.expm1(y_test)\n",
    "    test_r2 = r2_score(y_test_actual, y_test_pred)\n",
    "    test_mae = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "    print(f\"Final Testing Accuracy (R²): {test_r2:.2%}\")\n",
    "    print(f\"Final Testing MAE: {test_mae:.2f}\")\n",
    "    \n",
    "    # --- Visualization ---\n",
    "    # Pass the actual, untransformed values to the plotting function\n",
    "    plot_actual_vs_predicted(y_train_actual, y_train_pred, visualization_dir, data_set_name='Training')\n",
    "    plot_actual_vs_predicted(y_test_actual, y_test_pred, visualization_dir, data_set_name='Testing')\n",
    "    \n",
    "    # --- Save the Model ---\n",
    "    if not os.path.exists(model_output_dir):\n",
    "        os.makedirs(model_output_dir)\n",
    "    model_filepath = os.path.join(model_output_dir, 'mepfs_budget_model_lgbm_tuned.joblib')\n",
    "    joblib.dump(best_model, model_filepath)\n",
    "    print(f\"\\nTrained and tuned model saved to {model_filepath}\")\n",
    "\n",
    "MODEL_OUTPUT_DIR = 'models'\n",
    "train_budget_prediction_model_advanced(df_featured, MODEL_OUTPUT_DIR, VISUALIZATION_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
